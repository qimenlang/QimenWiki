**Detector/Invoker框架搭建，实现多交互器对单一交互对象的交互**

- 需求讨论
  - Detector和Invoker拆分为两个component，独立工作，二者均对应一个device；
  - Invoker在创建时保留Detector的引用，建立连接关系，二者为1:1或者2:1的关系；
  - Invoker处理事件，产生event args需要包含两个设备信息：detect device和invoke device，传给应用侧 ；在手柄、手势等交互中，detect device和invoke device实质对应同一个device，在手眼交互中，detect device和invoke device分别对应眼部设备和手部设备；
  - Invoker、detector需要响应设备的连接、断开；
- 框架设计
  - 由于Detector和Invoker必须相互协作，不能单独使用，依然使用一个Component，不做拆分，该Component中有DetectorManager和InvokerManager两个管理器；
  - 为每个window/client端进程，在scene中增加一个scene object，该scene object的eventcaster负责所有选择器、触发器的交互逻辑；
  - DetectorManager注册、管理所有选择器，InvokerManager注册、管理所有触发器，二者是生产者消费者关系，DetectorManager输出{Candidate、Device}；
  - 为支持多触发器对同一candidate的交互，InvokerManager记录每个触发器正在进行的handler，以便双手交互发生时，进行回退；
  - 检测触发方式
    - 先检测后触发 ： 由检测器检测交互对象，等待触发器的触发信号（轴数据更新），之后对交互对象触发事件；
    - 检测即触发 ： 检测器检测到交互对象，立即触发事件，例如：近远场hover，近场click、press、long press事件；
  - 检测器需要支持‘检测即触发’的事件
    - Hover事件
    - 手势近场交互方案：
      - client端检测触发：client端碰撞检测后，立即触发click、press、longpress等事件，检测、触发完全在client端的detector进行；
      - server/client均进行检测，server端检测之后触发轴更新，client端收到轴数据之后，对client端检测对象进行交互；
        - 存在因client端相对server端有一定延迟，导致的两端检测结果数组可能不一致，轴数据同步时机不确定等问题；
  - server端检测、client端执行 相关问题
    - 目标：server端进行检测交互窗口、交互对象，对所有交互对象result进行座标系转换，转换到client端，client端不再进行交互对象检测；
    - server 输出 交互对象和轴数据到client端；
    - 问题：
      - 轴数据 到android window，会不断重复，到非android window 不会重复，需要统一？
      - 眼手交互中 android window侧拿到server端圆锥检测的结果，可能只检测了一个board形状的collider,android侧依然无法实现对内部各个组件的范围选择；
    - 
- 事件互斥需求分析
  - 单设备事件互斥
    - translate事件cancel hover (其他事件也可以有同样需求)
    - translate事件cancel hover，影响slider效果实现
    - scroll事件cancel translate\hole\press\long press等事件
  - 多设备事件互斥
    - 一个交互对象在被多个设备hover期间，只触发第一个设备的hover事件；
    - 设备对交互对象press down期间，交互对象被独占，其他设备无法交互；
    - 多设备交互，例如 双手scale触发后，cancel交互对象已被触发的所有事件；
- Android 事件交互梳理
  - WindowImpl::intersects

需要先完成server端检测交互对象并座标系转换到client端的重构

眼手交互问题梳理

1. 交互场景

   - android应用交互：	

     一阶段：server端进行检测交互窗口、交互对象、得到交点数据，交点数据座标系转换到client端，client端如果是android应用，则安卓引用直接使用该交点，其他类型client端应用依然自行检测交互对象；

     二阶段：**server端进行检测交互窗口、交互对象，对所有交互对象result进行座标系转换，转换到client端**，client端不再进行交互对象检测；

     系统组需要支持眼手对android应用的交互，对于手势设备的轴数据更新，使用眼设备的检测交点信息做范围选择\最佳候选计算；

   - 近场交互 : 近场交互poke检测即触发（不需要轴数据作为信号量），之前的实现方式是server端检测后更新轴数据到client端，client端也同步检测；
   - 全屏模式交互 ：目前存在全屏模式下，手势mesh已更换scene，但无法交互的问题，可能eventcaster需要切换scene;

2. 交互切换

   - 交互方式切换：手柄、眼手、HMD；
     - 静态切换
     - 动态切换
   - 近远场切换 : 架构设计上支持，目前近场交互未实现；

3. 射线交互切换到detector/invoker架构；

4. 事件屏蔽互斥：双手事件cancel单手事件，单手事件独占交互对象，hover事件共享交互对象；单设备触发事件：Scroll打断hold\translate\press\long press等，translate事件cancel hover事件；

5. 提供算法参数配置文件 

6. 目前存在的bug

   - launcher无法用手势滑动 ， 需应用组适配 ： project/vivo-bug#2219 
   - 手势识别问题 : project/vivo-bug#2363、project/vivo-bug#1783
   - android交互 : project/vivo-bug#2333、project/vivo-bug#2070、
   - 体验问题 ： project/vivo-bug#2393、project/vivo-bug#2330、project/vivo-bug#2117、project/vivo-bug#1791
   - 性能问题： project/vivo-bug#2296、

   

眼手TODO

- [ ] Server 用双圆锥检测，给 client 提供模拟射线数据，client 保持射线检测逻辑 （Android，js 通用） 【Server 做一次检测的方案往后放，后续再重构】   【1-2天】
- [ ] 手势直接交互，Server 端做直接交互的检测，然后模拟射线数据给 client 【1天】  
- [ ] 手势近远场切换逻辑 【1天】
- [ ] 全屏 Caster bug  【1-2小时】
- [ ] 性能问题，手势蒙皮方案修改  Zhang Haiyue 
- [ ] 算法参数配置【2小时】

这周完成以上内容，下周开始调具体交互效果
我们自己写一个 js Demo，覆盖所有测试场景，不要依赖应用组现有应用的配合

server端进行双圆锥检测和近场检测，近远场切换，远场时使用圆锥焦点和手腕关节点确定射线pose,近场时使用食指顶点和手腕关节点确定射线pose；client端只做射线检测；捏合之后，射线rotation不再移动；

眼手远程交互 射线rotation Mapping方案：

- pinch之后，射线rotation受手势位移影响、窗口大小影响；
- pinch之后，射线rotation先固定，后续受index\手腕两个关节点射线方向的变化量控制；

BUG：

- [ ] 替换射线后，hover效果的交互对象不一定真正可以交互；（手pinch时射线的抖动影响）
- [ ] 手势未检测时，眼动的目标对象无hover效果；
- [ ] 手势近场交互：ScrollView滑动效果不对；
- [ ] 已虚化的窗口，无法通过点击focus

Server端检测、client端模拟射线

- 目标点在collider边缘时，手势pinch动作导致设备pose轻微变化，client端设备不一定能检测到交互对象；
- client端使用射线，checkAim也回退到射线，不容易选择交互对象；

server端检测结果传递到client端方案，眼手交互自测bug：

- [ ] 2D UI 无法交互
- [ ] 3D ScrollView 无法滑动
- [ ] CSE move bar点击会崩，close bar点击后 window消失（不确定崩溃还是正常关闭）
- [ ] android应用、近场交互





```c++
FocusEventArgs{
    EventType;//Get、Lost
    EventCaster
}

HoverEventArgs{
	EventType;//begin、update、end
	InputDevice;
	DetectorInputDevice;
	RaycastResult;
	EventCaster;
}

// AxisEvent、ClickEvent
AxisEventArgs{	
	InputAxis;
	InputDevice;
    DetectorInputDevice;
	RaycastResult;
	EventCaster;
}

HoldEventArgs：public AxisEventArgs{
    EventType;//begin、update、end、cancel
}

//Press、LongPress
PressEventArgs：public AxisEventArgs{
    EventType;//begin、update、end、cancel
}

TranslateEventArgs：public AxisEventArgs{
    EventType;//begin、update、end
    Pose;//目标物体预计移动到的位姿
}

ScrollEventArgs：public AxisEventArgs{
    EventType;//begin、update、end、cancel
    offset;//滑动速度的变化量
}

DragStartEventArgs{
    DataPackageOperation;//Empty、Forbid、Copy、Move、Open
    DragDataPackage;//SceneObjecy、TXT、Image
    GuiData；
}

DragEventArgs{
    DataPackageOperation acceptedOperations;
    DataPackageOperation allowedOperations;
    DragDataPackage;
    GuiData;
}

DropCompletedEventArgs{
    DataPackageOperation dropResult;
}


```







